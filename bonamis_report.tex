\documentclass[a4paper, 11pt]{report}

\usepackage{shorttoc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}

\begin{titlepage}
    %--------------------------
    % Page de garde du rapport
    %--------------------------
    \parindent=0pt
    \hrulefill
    \begin{center}\bfseries\Huge
        Modeling of the shape of proteins from SAXS data
    \end{center}

    \hrulefill
    \vspace*{1cm}
    \begin{center}\bfseries\Large           %author
        Guillaume Bonamis
    \end{center}

    \begin{center}\bfseries\Large           %supervisor
        Supervisor: J\'er\^ome Kieffer
    \end{center}

    \vspace*{\stretch{2}}
    \begin{flushright}
        August 27, 2015
    \end{flushright}   
    
    %rajouter un mot sur le tuteur Lille1
\end{titlepage}


\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
\pagenumbering{Roman}
    %-------------------------------------
    % Page pour les divers remerciements
    %-------------------------------------


\tableofcontents
\addcontentsline{toc}{chapter}{Table of contents}
\pagenumbering{arabic}


\chapter{Introduction}
    %-------------------
    % Premier chapitre   
    %-------------------

\section{The European Synchrotron Radiation Facility}

The ESRF is a user facility providing very intense X-rays beams to scientists to
perform absorption, diffraction or spectroscopy experiments.
The X-rays in a synchrotron are generated by electrons travelling at 
99.9999\% of the speed of light, inside a long, circular tube in near-
perfect vacuum.

At the ESRF, these electrons are first accelerated by a 16-metre-long 
linear accelerator (linac) before entering a small, racetrack shaped 
booster accelerator. 
Once they have reached their final speed (at an energy level of 6 GeV), 
these high-energy electrons are injected into the vacuum tube of the
844-metre-long storage ring.
Here they are guided on their orbital path by magnets. 
In between these magnets, the electrons pass through insertion 
devices, also called undulators.
During each passage, the electrons release bursts of intense X-rays, 
along with electromagnetic radiation with other wavelengths, from 
infrared light to gamma-rays. 
These X-ray bursts are projected in the forward direction, like a 
laser beam thin as a human hair (0.1 mm diametre).

The synchrotron X-ray beam leaves the main storage ring at a point a 
few metres after the undulator. 
Undulators are placed at 30 positions around the storage ring.
Upon leaving the storage ring, the X-rays enter one of 41 beamlines, 
each an ensemble of laboratory blocks or “hutches” where the actual 
research takes place. 
The figure~\ref{fgr:synchrotron} sum this running.

\begin{figure}
\centering
\includegraphics[scale=0.22]{synchrotron.png}
\caption{Chart of the ESRF with its main components, linac, booster, 
    storage ring and beamlines}
\label{fgr:synchrotron}
\end{figure}

\section{BioSAXS beamline BM29}
\label{bm29}
BM29 \cite{BM29paper} is a beamline for Small Angle X-ray Scattering 
(SAXS) experiments of biological macromolecule solutions with the goal 
to determine their 3-dimensional structures in a natural state with a 
'low' resolution (few nm). 
The beamline data acquisition running is based on a pipeline of 
individual tasks, highly automated, from the inputs to the final 
results. 
Here will be shortly described this series of process, focussing on 
what matter for our subject.
All data treatments have recently been described in \cite{BM29news} 
(submitted to \textit{Journal of Applied Crystallography}).

The series of tasks is controlled by EDNA, which is a plugin-based 
framework dedicated to pipelines data-analysis building \cite{edna}. 
It begin with the acquisition of the 2-d scattering image of the 
studied sample by a detector, a Pilatus 1M for BM29. 
Then, the global data-analysis process can be divided in four steps: 
\begin{itemize}
 \item azimuthal integration of the image
 \item background correction (scattering of the sample environment)
 \item basic analysis of the scattering curve
 \item ab-initio modelling
\end{itemize}

\begin{figure}
\centering
\includegraphics[scale=0.3]{schemaSAXS.png}
\caption{Schema of a SAXS experiment, scattering of the incident beam 
    create the scattered image on the detector}
\label{fgr:schemaSAXS}
\end{figure}

The image obtained represents the X-ray scattering by the sample, it 
has a cylindrical symmetry along the scattered beam, as seen on 
figure~\ref{fgr:schemaSAXS}. 
The first step consists in averaging the 2-d image along the 
symmetry axis (also called azimuthal integration).  
This operation is performed using FabIO \cite{fabio} for the image 
reading and PyFAI \cite{pyFAI} for the azimuthal integration and the 
result is a curve representing the scattered intensity function of the 
scattering vector $q = 4 \pi \frac{sin(\theta)}{\lambda}$ (expressed 
in inverse nanometers).

This curve has to be background-corrected: an EDNA plugin subtracts the
scattering intensity of the sample environment, recorded using a buffer
solution, to the sample curve, resulting in only the scattering signal of the
macromolecule. The background-corrected curve is then analysed with several
tools from the \textsc{atsas} package \cite{atsas}. 
%Dammif is it using one of these curves ???
The EDNA analysis pipeline generates automatically Guinier and Kratky
analysis plots for visual analysis of the scattering signal.

The fourth step of the data-analysis is the ab-initio modelling, 
presented in the following section, which is the core of the subject.

\section{Ab-initio modelling}
\label{modelling}                           %flag pour citer cette section

Ab-initio modelling aims at building a low-resolution 3-d  
model of the macromolecule of interest. 
For it, BM29 uses other tools from the \textsc{atsas} package as 
follow.

The first step of the modelling consists in reconstructing many ($N$, ususally 8
or 16) 3-d dummy-atoms models from the background-corrected curve with 
\textit{dammif} \cite{dammif}. %really that curve ???
These dummy-atoms models (DAM) are then selected according to two 
parameters computed with the curve: 
\[
R = \frac {\sum {||I_{obs}| - |I_{calc}||}}{\sum {|I_{obs}|}}; \ \ \ 
\chi^{2} = \sum {\frac {(I_{obs} - I_{calc})^{2}}{\sigma^{2}}}
\]
These two values allow to evaluate the goodness of the fit. 
The thresholds are respectively the mean plus one or two standard 
deviation, and models out of range are simply discarded.

Remaining DAM are then superimposed two by two using the \textit{supcomb}
\cite{supcomb} program which reports the Normalized Spatial
Discrepancy (NSD), a metric to evaluate the difference between two
models.
It allows the segregation of similar models and the others. 
So, once again, outlier models are discarded based on the NSD
values.
For each DAM, the mean of its NSD with others is computed; 
if this value exceeds the mean plus two time the standard 
deviation, it is discarded. 
The DAM with the lowest value is the reference model.

So here, at the moment of the process, $N - n$ models remain and one 
of them is the reference. 
All these $N - n$ models are merged with \textit{damaver} 
\cite{damaver}. 
Subsequently \textit{damfilt} gets the average DAM (which is not solution of
the problem) and \textit{damstart} generates an input model for 
\textit{dammin} \cite{dammin} which refines the model to fit the 
subtacted curve. 
The outcome of the Monte-Carlo refinement performed by \textit{dammin} is the
final model which can be interpreted by biologists (comparison with fragments
coming fron NMR or comparison with other technics).

\section{Traineeship subject}

The topic of the traineeship was to work on the part "superimposition 
and selection of the models" of the modelling process. 
In fact, all the modelling process was pointed out as the most 
problematic according to the beamline scientists for several reason:
\begin{enumerate}
\item the BM29 pipeline is based on Python whereas most
of the \textsc{atsas} package is written in Fortran, so for a better 
integration in the pipeline, a Python library could be advantageous. 
\item programs from this package are closed source and 
changing from version to version add difficulties for their 
integration in the pipeline. 
\item the execution time of these programs make the ab-initio modelling the
bottleneck of the BM29 pipeline. 
\item sometimes the whole EDNA pipeline needs to be aborted 
due to \textit{dammin}, not returning after half an hour.
\end{enumerate}

In accordance with these observations, it was decided to create a 
homemade package to overpass these data-analysis problems. 
These package has been called FreeSAS, is open source and free (under 
the MIT License). 
Written in Python, it aims at providing tools for the BioSAXS data 
analysis, with an execution time optimized and providing reliable 
results. 
The subject was dedicated to this Python package, and especially the 
re-implementation of the superimposition of the DAM (so previously 
done by \textit{supcomb}) and the selection of those to keep along the 
process.

In the following chapter, we will describe all the implementation 
computed, and the theory behind them, to reach our aim. 
Then, in chapter 3, ....... %a preciser plus tard


\chapter{FreeSAS implementation}
    %--------------------
    % Deuxieme chapitre  
    %--------------------

%TODO rewrite the intro

As mentioned previously, FreeSAS is a Python package for BioSAXS data-
analysis. 
It is built of several modules, each focusing on a special task. 
\begin{itemize}
 \item the physical theory behind this problem of DAM superimposition 
and selection
 \item the different Python tools and algorithms used to reach our aim
\end{itemize}
The last section will describe the way all these tasks are exploited 
to create a "global process" computing the wanted result. 

\section{Reading inputs data}

The library has to be able to work within the BM29 pipeline presented 
in section \ref{bm29} and used in production for a few years  now. 
As a component of a larger piece of software, it has to deal with inputs 
coming from other software, especially the \textit{atsas} programs used
upstream and needs to provide outputs compatible with \textit{atsas}, also used
downstream.
Here we will focus on the reading of the input structures and describe the
ones we are interested in.

The inputs of the superimposition module is the
outputs DAM of the various \textit{dammif} runs. 
These structures are written in files using the PDB format (for 
Protein Data Bank), a standard representation of 
macromolecular structure data.\\ 
Berman, Helen M. "The protein data bank: a historical perspective." Acta
Crystallographica Section A: Foundations of Crystallography 64.1 (2007):
88-95.\\
The (simplistic) parser written copes for two things: reading the dummy atoms'
coordinates (beads) and retriving the quality of the fit of the model on the
experimental data ($\chi^2$ and $R$-value).
Those numbers represent the quality of the model, calculated using the 
difference between the actual SAXS curve and the simulated one from the
corresponding model. 
Those goodness of fit values are only used to reject model badly fitting
experimental data.

The second piece of information to retrieve is the dummy-atoms coordinates. 
As already mentioned, only low-resolution information are available in SAXS data 
hence the structure of model is made of dummy-atoms.
They are not actual atoms, but indicate there is 
some material (the macromolecule) at their positions, and where there is
no dummy-atoms, it is solvent. 
Typical DAM used in this project had 500 dummy-atoms (order of magnitude).
The parser written was dedicated to these task: to read the pdb file 
to select the data we needed, and then to keep in memory in one hand 
the $R-value$ of the DAM and in an other hand the Cartesian coordinates (x, y, 
z, in Angstrom) of each dummy-atoms of the model. 
These coordinates are stored as a 2-d numpy array from the 
\textit{NumPy} package for scientific Python \cite{numpy}; they are 
necessary for geometric handling we will have to do along our process.

\section{Similarity calculation}

To be able to merge similar models, and reject dis-similar ones,
one needs to define the similarity of two models. 
This section introduces a ``distance'' to measure this  
similarity and describes its implemention in the FreeSAS
package.

The likeness of two 3-d models can be, at first glance, very 
subjective. 
To be able to compute it, we use a metric called Normalized Spatial 
Discrepancy (NSD), introduced by Kozin and Svergun in their paper 
\textit{Automated matching of high- and low-resolution structural 
models} \cite{supcomb}. 
This value quantifies the dissimilarity between two sets of 
point $S_{1}$ and $S_{2}$, in our case in a three-dimensionnal space.
Those sets are neither ordered nor of same size ($N_{1}$ points for
$S_{1}$ and $N_{2}$ points for
$S_{2}$), making useless other techniques like Kabsch method
\cite{kabsh1976}.
The NSD has been built has an analogue to the standard Euclidian distance,
but as it is normalized, it is not homogeneous to a distance:

\[
\rho(S_{1},S_{2})= \frac{1}{2} \sqrt {\frac{1}{N_{1}d_{2}^2} 
\cdot \sum\limits_{i=1}^{N_{1}} \rho^2(s_{1i}, S_{2}) + \frac{1}{N_{2}d_{1}^2} \cdot \sum\limits_{i=1}^{N_{2}} \rho^2(s_{2i}, S_{1})}
\]

For a point $s_{1i}$ in $S_{1}$, the Euclidian distance with all the 
points of $S_{2}$ is computed, and the minimal one is denoted as 
$\rho(s_{1i}, S_{2})$. 
The last parameters present in the formula is the fineness $d_{i}$. 
It correspond to the average Euclidian distance between a point of 
$S_{i}$ and its nearest neighbour.\\
The NSD can be used to measure the similarity of two sets of points as:
\begin{itemize}
 \item it is symmetric, $\rho(S_{1},S_{2}) = \rho(S_{2},S_{1})$
 \item it is stable, $\rho(S_{1},S_{2})$ function has a stable minimum
 \item $\rho(S_{1},S_{2}) = 0$ only if $S_{1} = S_{2}$
\end{itemize}

As the calculation of the NSD is the core of our process, it is of prime  
importance to optimize its implementation. This has been performed in 3
subsequent steps:
\begin{enumerate}
  \item numpy implementation calculating the 2d table of distances from all
  atoms in $S_{1}$ to any atoms in $S_{2}$.
\end{enumerate}

The first thing done for it was to re-wrote the calculation of the NSD 
in Cython \cite{cython}. 
This programming language translates Python code to an equivalent C 
code which allows major gain in term of execution time.

To have again speed improvements, a second step has been performed: 
this piece of code has also been parallelized with OpenMP. 
Indeed the parallelization of some calculations allows to take 
advantage of the presence of a multi-core processor on the computer, 
which is the case on the computers used on the beamline BM29.

\section{Coarse superimposition of models}

With the data extracted of pdb files and the NSD tool, we should be 
able to calculate the dissimilarity between two models. 
The fact is that we need other tools for it because of the arrangement 
of the dammif models. 
Actually the DAM we dispose are randomly oriented, so at this moment 
of the data-analysis, the NSD between two models do not show their 
difference but only the spatial distance that separates them. 
That is why we have to first superimpose the models so that the NSD 
only point out the dissimilarity. 
Here we will define the superimposition algorithm followed in our 
program with parameters needed for it.\\

To superimpose two DAM with their set of dummy-atoms $S_{1}$ and 
$S_{2}$, we have used the algorithm defined in Kozin and Svergun 
publication \cite{supcomb} as a three-steps process.\\

The first stage consists in superimposing both models center of mass 
(COM). 
In the dummy-atoms modelization, all the dummy-atoms has the same mass 
so the COM coordinates of the model $k$ are computed as follow:
\[
x_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} x_{ik};\ \ \ 
y_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} y_{ik};\ \ \ 
z_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} z_{ik}
\]\\
From it, the translation to send the model $S_{k}$ to the coordinates 
origin is calculated as $T_{k}^0 = (-x_{k}^0,\ -y_{k}^0,\ -z_{k}^0)$. 
Thus, to send $S_{2}$ on $S_{1}$, we apply it two transformations: 
$T_{2}^0$ first and $(T_{1}^0)^{-1}$, where $(T_{1}^0)^{-1}$ is the 
reverse of the translation $T_{1}^0$.\\

Next come the alignment stage. 
To refine the superimposition of the two models, a rotation is 
performed on the second one. 
This rotation aims to align the principles axis of inertia of both 
DAM. 
To compute them, we first need to know the inertia matrix of the 
models $S_{k}$; and it is given by:
\[
I_{k}=
\begin{pmatrix}
 I_{11} & I_{12} & I_{13} \\
 I_{21} & I_{22} & I_{23} \\
 I_{31} & I_{32} & I_{33} \\
\end{pmatrix}
\]\\
and knowing that each point of the set $S_{k}$ is defined by
$
s_{kq}=
\begin{pmatrix}
 x_{kq}^1 \\
 x_{kq}^2 \\
 x_{kq}^3 \\
\end{pmatrix}
$\\
we have
\[
I_{ij} = \frac{1}{N_{k}} \cdot \sum\limits_{q=1}^{N_{k}} 
[\delta_{ij} \cdot \sum\limits_{l=1}^3 
 (x_{kq}^l - {x_{kq}^l}^0)^2 -  (x_{kq}^i - {x_{kq}^i}^0) 
 \cdot (x_{kq}^j - {x_{kq}^j}^0)]
\]\\
It appears that $I_{k}$ tensor is symmetric real so we know that it 
can be diagonalized; we can compute its three eigenvalues 
$\lambda_{k1} \geq \lambda_{k2} \geq \lambda_{k3}$ and the three 
eigenvectors corresponding $v_{k1},\ v_{k2},\ v_{k3}$. 
We can create a rotation matrix compound by the columns from 
eigenvectors of $I_{k}$, sorted with rising order of associated 
eigenvalues. 
If this matrix is wrote $M_{k}$, is transposed $[M_{k}]^T$ is the 
rotation matrix which align $S_{k}$ axis of inertia with the 
coordinates axis \cite{supcomb}.\\
With these rotation matrices, we are able to align two models 
following $x,\ y,\ z$ coordinates axis.\\

Before to be able to superimpose correctly $S_{2}$ on $S_{1}$, we need 
to take into account enantiomorphs. 
In fact, two enantiomorphs of a macromolecule generate the same SAXS 
curve, so that this method do not allows to segregate them. 
Thus, we do not make a difference for our data-analysis between two 
enantiomorphs; and it needs to be implemented. 
As we superimpose $S_{2}$ on $S_{1}$, we will force $S_{2}$ to be in 
the same enantiomorphic form that $S_{1}$. 
For it, $S_{2}$ and $S_{1}$ are set in their canonical position: 
center of mass and the origin of axis and inertia axis aligned with 
coordinates axis. 
Eight transformations are performed on $S_{2}$ which correspond to the 
eight possible symmetries, four for each enantiomorph. 
Transformation matrices are as follow:
\[
\begin{pmatrix}
 \pm 1 & 0 & 0 & 0 \\
 0 & \pm 1 & 0 & 0 \\
 0 & 0 & \pm 1 & 0 \\
 0 & 0 & 0 & 1
\end{pmatrix}
\]\\
And after each transformation, the NSD between $S_{1}$ and $S_{2}$ is 
computed. 
The one minimizing the NSD is kept as the good symmetry for the 
following steps. 
The rotation matrix for $S_{2}$ is now the previous one $[M_{2}]^T$ 
multiplied by the symmetry matrix, we will write it $R_{2}$.\\

In our work we chose to always align the second DAM on the first one 
initial position. 
We saw before that first $S_{2}$ is translated to put its center of 
mass on the one of $S_{1}$ using the translation 
$(T_{1}^0)^{-1} \times T_{2}^0$. 
In fact, the rotation and enantiomorph selection steps has to be 
inserted between these two translations. 
Finally, the global transformation to apply to $S_{2}$ is 
\[(T_{1}^0)^{-1} \times M_{1} \times R_{2} \times T_{2}^0\]

At the end of this process, we have two DAM whose centers of mass and 
principles axis of inertia are superimposed; we will say canonically 
superimposed.

\section{Refined superimposition}

The canonical superimposition presented before produce a good fit of 
the two models (if they are similar), but it remains a coarse 
alignment we need to optimize for a better understanding of the 
dissimilarity between two DAM. 
The core of the superimposition process remains the same, we will add 
on it the following refinement algorithm.\\
 
We have to optimize the transformation applied to $S_{2}$ to align it 
with $S_{1}$, and the criterion of improvement will be the NSD, which 
has to be as low as possible. 
For it, we use the simplex optimizer provided by the \textit{SciPy} 
library \cite{scipy}. 
But to use it, accurate numerical parameters are needed. 
What we did is to extract from the global transformation matrix 
presented before six transformation parameters: 
$(x_{0}, y_{0}, z_{0})$, 
corresponding to the total translation applied to $S_{2}$, and 
$(\alpha, \beta, \gamma)$, 
which are the Euler angles extracted from the total rotation. 
The optimizer receives this six parameters and call a function which 
returns the NSD between $S_{1}$ and $S_{2}$ after the input 
transformation has been applied to $S_{2}$. 
This call is repeat again and again, refining each time the six 
parameters values to minimize the NSD returned.\\
When these iterations reach a landing, the optimizer is stopped and 
returns the six refined parameters and the minimized NSD. 
Here, the superimposition of the two models is optimized, the final 
NSD value is assumed to reflect the actual dissimilarity between them.\\

\section{Dummy atoms models selection}

The algorithm to superimpose two DAM has been fully described, it has 
to be implemented in order to superimpose the $N$ models outgoing of 
\textit{dammif}. \\

For it, dammif pdb files are first read with the FreeSAS parser and 
usefull data are extracted as presented before. 
Some properties we introduced are then computed from it. 
At this stage we dispose of $N$ detailled DAM and all we need to 
superimpose them.\\
The superimposition is implemented to obtain a NSD table $N*N$ sized. 
Each model is represented by one row and one column. 
Each bow in the table will contain the optimized NSD between the "row" 
model and the "column" model. 
To resume, we want a $N*N$ table with 
\[
table_{ij}=NSD(S_{i},S_{j})
\]\\ 
We know that all we will have zeros on the diagonal and the table will 
be symmetric because of two properties of the NSD:
\[
NSD(S_{i},S_{i})=0.00 \Rightarrow table_{ii}=0.00
\]
\[
NSD(S_{i},S_{j}) = NSD(S_{j},S_{i}) \Rightarrow table_{ij}=table_{ji}
\]\\
Thus we can compute $\frac{N \cdot (N-1)}{2}$ optimized 
superimpositions instead of $N*N$.\\

Once the NSD table generated, we dispose of all we need to select the 
dummy-atoms models.\\
The first criteria of selection have already been presented in 
\ref{modelling} section: the $\chi^2$ and the $R-value$. 
These two parameters are not dependent of the DAM position, they are 
computed before any movements. 
So for a gain of execution time, the test on them are computed before 
the superimposition process. 
Thus some models (say $x$) are discarded before to be superimposition, 
which allow us to skip the optimized alignment calculation to save 
time: $\frac{(N-x) \cdot (N-x-1)}{2}$ instead of 
$\frac{N \cdot (N-1)}{2}$.\\
The following criterion of selection has also been explained in 
\ref{modelling} section, the average NSD between each models and 
others. 
This variable is computed using the NSD table obtained by the process 
we know and the test is performed for each DAM remaining ($N-x$ at 
this level). 
A new series of models is discarded (or neither), that give us the 
$N-n$ acceptable DAM to follow the modelling process of EDNA. 
Moreover, the model with the lower average NSD to all other valid 
models is set as "reference model".\\

At the end of it, data are formatted to present correctly the analysis 
results. 
First, what concern DAM is saved using PDB format to be accessible to 
the next program in the pipeline: \textit{damaver}. 
The models are aligned on the reference one and are saved in this 
position, as needed by \textit{damaver}. 
Informations about the selection process are also saved to be 
available for the BM29 user. 
A piece of code has been written to create figures with $\chi^2$ and 
$R-value$ values and thresholds, the NSD table and the average NSD 
values and threshold. 
These figures are computed using the \textit{matplotlib} Python 
library \cite{matplotlib}.\\

We saw in this chapter the way FreeSAS package has been implemented in 
order to superimpose and select dummy-atoms models from 
\textit{dammif}. 
FreeSAS provides tools to read and compute data from PDB files and to 
process them using several algorithms. 
At the end, it returns the valid DAM, superimposed and ready to be 
used by \textit{damaver}, and all data usefull for the users.
%Ajouter ici, brievement, ce qui va etre dit dans le prochain chapitre !!!


\chapter{Pratical example}%titre temporaire
    %---------------------
    % Troisieme chapitre  
    %---------------------

\section{section nb 1}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.

\section{section nb 2}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.

\section{section nb 3}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.


\chapter*{Conclusion and outlook}
\addcontentsline{toc}{chapter}{Conclusion and outlook}
    %----------------------------------------
    % Chapitre de conclusion + perspectives
    %----------------------------------------


\newpage                 %ne pas enlever (numerotation pour sommaire)
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}%ou autre ???
\bibliography{bibliography}
    %-------------------------
    % Bibliographie du stage
    %-------------------------


\appendix
    %---------------------
    % Annexes du rapport
    %---------------------

%a voir pour le contenu

\chapter{First appendix}

\chapter{Second appendix}


\end{document}

