\documentclass[a4paper, 11pt]{report}

\usepackage{shorttoc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\begin{document}

\begin{titlepage}
    %--------------------------
    % Page de garde du rapport
    %--------------------------
    \parindent=0pt
    \hrulefill
    \begin{center}\bfseries\Huge
        Modeling of the shape of proteins from SAXS data
    \end{center}

    \hrulefill
    \vspace*{1cm}
    \begin{center}\bfseries\Large           %author
        Guillaume Bonamis
    \end{center}

    \begin{center}\bfseries\Large           %supervisor
        Under the supervision of J\'er\^ome Kieffer
    \end{center}

    \vspace*{\stretch{2}}
    \begin{flushright}
        August 27, 2015
    \end{flushright}   
    
    %rajouter un mot sur le tuteur Lille1
\end{titlepage}


\chapter*{Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}
\pagenumbering{Roman}
    %-------------------------------------
    % Page pour les divers remerciements
    %-------------------------------------


\tableofcontents
\addcontentsline{toc}{chapter}{Table of contents}
\pagenumbering{arabic}


\chapter{Introduction}
    %-------------------
    % Premier chapitre   
    %-------------------

\section{The European Synchrotron Radiation Facility}

The X-rays in a synchrotron are generated by electrons travelling at 
99.9999\% of the speed of light, inside a long, circular tube in near-
perfect vacuum.\\

At the ESRF, these electrons are first accelerated by a 16-metre-long 
linear accelerator (linac) before entering a small, racetrack shaped 
booster accelerator. 
Once they have reached their final speed (at an energy level of 6 
billion electron volts, or 6 GeV), these high-energy electrons are 
injected into the vacuum tube of the 844-metre-long storage ring. 
Here they are guided on their orbital path by magnets. 
In between these magnets, the electrons pass through insertion 
devices, also called undulators.\\
During each passage, the electrons release bursts of intense X-rays, 
along with electromagnetic radiation with other wavelengths, from 
infrared light to gamma-rays. 
These X-ray bursts are projected in the forward direction, like a 
laser beam thin as a human hair (0.1 mm diametre).\\

The synchrotron X-ray beam leaves the main storage ring at a point a 
few metres after the undulator. 
Undulators are placed at 30 positions around the storage ring.\\

Upon leaving the storage ring, the X-rays enter one of 41 beamlines, 
each an ensemble of laboratory blocks or “hutches” where the actual 
research takes place. 

%insert there an image to present storage ring and emission of light ???

\section{BioSAXS beamline BM29}

BM29 \cite{BM29paper} is a beamline for Small Angle X-ray Scattering 
(SAXS) experiments of biological macromolecule solutions with the goal 
to determine their 3-dimensional structures in a natural state with a 
'low' resolution (few nm). 
The beamline data acquisition running is based on a pipeline of 
individual tasks, highly automated, from the inputs to the final 
results. 
Here will be fastly described this series of process, focussing on 
what matter for our subject.
All data treatments have recently been described in \cite{BM29news} 
(submitted to \textit{Journal of Applied Crystallography}).\\

The series of tasks is controlled by EDNA, which is a plugin-based 
framework dedicated to pipelines data-analysis building \cite{edna}. 
It begin with the acquisition of the 2-d scattering image of the 
studied sample by a detector, a Pilatus 1M for BM29. 
Then, the global data-analysis process can be divided in four steps: 
\begin{itemize}
 \item azimuthal integration of the image
 \item background correction
 \item basic analysis of the curve
 \item ab-initio modelling
\end{itemize}

The image obtained reprensents the X-ray scattering by the sample, it 
has a cylindrical symmetry along the transmitted beam. 
The first step consists in integrating the 2-d image along the 
symmetry axis. 
This operation is performed using FabIO \cite{fabio} for the image 
reading and PyFAI \cite{pyFAI} for the azimuthal integration and the 
result is a curve representing the scattered intensity function of the 
scattering vector $q = 4 \pi \frac{sin(\theta)}{\lambda}$ (expressed 
in inverse nanometers).\\

This curve has to be background-corrected using the scattering of the 
buffer solution , wich allows some EDNA plugins to subtract to the 
sample curve all that do not results of the scattering du to the 
sample. 
The background-corrected curve is then analysed with several tools 
from the \textsc{atsas} package \cite{atsas}. 
%Dammif is it using one of these curves ???
It creates automatically several curves that we will not describe here 
because they do not contribute to the pipeline we are interested in.\\

The fourth step of the data-analysis is the ab-initio modelling, 
presented in the following section.

\section{Ab-initio modelling}
\label{modelling}                           %flag pour citer cette section

Ab-initio modelling aims to retrieve a 3-d low-resolution structural 
model of the input molecule. 
For it, BM29 use other tools from the \textsc{atsas} package as 
follow.\\

The first step of the modelling consists in reconstructing $N$ 3-d 
dummy-atoms models from the background-corrected curve with 
\textit{dammif} \cite{dammif}. %really that curve ???
These dummy-atoms models (DAM) are then selected according to two 
parameters computed with the curve: 
\[
R = \frac {\sum {||I_{obs}| - |I_{calc}||}}{\sum {|I_{obs}|}}; \ \ \ 
\chi^{2} = \sum {\frac {(I_{obs} - I_{calc})^{2}}{\sigma^{2}}}
\]
These two values allow to evaluate the goodness of the fit. 
The thresholds are respectively the mean plus one or two standard 
deviation, and out of range models are discarded.\\

Remaining DAM are then superimposed two by two and a metric called 
Normalized Spatial Discrepancy (NSD) is computed to measure the 
difference between two models. 
The program used here is \textit{supcomb} \cite{supcomb}. 
It allows to segregate similar models and others. 
So, once again, models are discarded, this time using the NSD values. 
For each DAM, the mean of its NSD with others is computed. 
And if this value exceeds the mean plus two time the standard 
deviation, it is discarded. 
The DAM with the lower value is said to be the reference model.\\

So here, at the moment of the process, $N - n$ models remain and one 
of them is the reference. 
These $N - n$ models are merged with two programs, \textit{damaver} 
\cite{damaver} and \textit{damfilt}, to get the average solution (an 
average DAM). 
The result from \textit{damaver} is then used by \textit{damstart} and 
\textit{dammin} \cite{dammin} to compute a refined model which fit the 
initial curve and so is assumed to be the wanted result: the 
structural model of the molecule from the studied sample.

\section{Traineeship subject}

The topic of the traineeship was to work on the part "superimposition 
and selection of the models" of the modelling process. 
In fact, all the modelling process was pointed out as the most 
problematic according to the beamline scientists for several reason.\\
The first one is that the BM29 pipeline is based on Python whereas the 
\textsc{atsas} package is wrote in Fortran, so for a better 
integration in the pipeline, a Python code could be advantageous. 
%atsas en Fortran ???
The fact that the programs from this package are closed source and 
changing from version to version add difficulties for their 
integration in the pipeline. 
An other reason come from the execution times of these programs which 
make the ab-initio modelling the bottleneck of the BM29 pipeline. 
Furthermore, it could happened that the process was aborted because of 
the last program launched, \textit{dammin}, which cannot finish its 
work after half an hour.\\

In accordance with these observations, it was decided to create a 
homemade package to overpass these data-analysis problems. 
These package has been called FreeSAS, is open source and free (under 
the MIT License). 
Written in Python, it aims at providing tools for the BioSAXS data 
analysis, with an execution time optimised and providing reliable 
results. 
The subject was dedicated to this Python package, and especially the 
re-implementation of the superimposition of the DAM (so previously 
done by \textit{supcomb}) and the selection of those to keep along the 
process.\\

In the following chapter, we will describe all the implementation 
computed, and the theory behind them, to reach our aim. 
Then, in chapter 3, ....... %a preciser plus tard


\chapter{FreeSAS implementation}
    %--------------------
    % Deuxieme chapitre  
    %--------------------

As presented previously, FreeSAS is a Python package for BioSAXS data-
analysis. 
It is compound of several modules, each focussing on a special task. 
Here is presented the implementation of FreeSAS, but gathering some 
modules in several workgroups to well explain both:
\begin{itemize}
 \item the physical theory behind this problem of DAM superimposition 
and selection
 \item the different Python tools and algorithms used to reach our aim
\end{itemize}

The last section will describe the way all these tasks are exploited 
to create a "global process" computing the wanted result. 

\section{Reading inputs data}

The code wrote has to be able to work in the BM29 pipeline presented 
in the previous chapter. 
As a piece of a large program, he has inputs comming from the program 
used before it and has to provide outputs which can be used by the 
program launched after it. 
Here we will focus on the reading of the inputs data and the ones we  
are interested in and why.\\

The inputs of the "box" we have to build are the outputs DAM of the 
\textit{dammif} program. 
These data we dispose are in files in a special format called PDB for 
Protein Data Bank. %citer quelquechose pour format PDB ???
The PDB format is used to provide a standard representation of 
macromolecular structure data. 
For the processing we will have to apply to these dummy-atoms model, 
we need to retrieve two things from the pdb file of the DAM: the atom 
beads' coordinates and the quality of the model.\\

The quality of the model is represented as the $R-factor$ and the 
$\chi^2$. 
Those numbers represent the quality of the model, calculated using the 
difference between the SAXS curve which could be retrieve if the real 
protein in the sample was this model and the actual curve obtained by 
SAXS experiment. 
These images of the goodness of the fit of this model is not used 
straightway in the data treatment but only in the selection part of 
the algorithm; so we will see how we use it in the fifth section.\\

The second data to retrieve are the dummy-atoms coordinates. 
As it has been mentioned, we want to retrieve the low-resolution 
structure of the molecule from the studied sample and for it we create 
a model with dummy-atoms. 
They are not actual atoms, they mean that, at this position, there is 
some material (the molecule), and where there is no dummy-atoms, it is 
solvent. 
Typically, the DAM we worked with were compound by 500 dummy-atoms (
order of magnitude).\\
The parser we wrote was dedicated to these task: to read the pdb file 
to select the data we needed, and then to keep in memory in one hand 
the $R-factor$ of the DAM and in an other hand the coordinates (x, y, 
z) of each dummy-atoms of the model. 
These coordinates are stored as a 2-d numpy array from the 
\textit{NumPy} package for scientific Python \cite{numpy}; they are 
necessary for geometric handling we will have to do along our process.

\section{NSD calculation}

Now we have to come back to what we saw at the end of the previous 
chapter and at the beginning of this one. 
We saw that our code has $N$ DAM as inputs and $N - n$ as outputs, and 
that the choice of the models to discard depend of the similarity of 
each model with the other. 
So we have to detail the way this similarity is measured and how it 
has been implemented in the FreeSAS package.\\

The likeness of two 3-d models can be, at first glance, very 
subjective. 
To be able to compute it, we use a metric called Normalized Spatial 
Discrepancy (NSD), introduced by Kozin and Svergun in their paper 
\textit{Automated matching of high- and low-resolution structural 
models} \cite{supcomb}. 
This tool allows to quantify the dissimilarity between two sets of 
point, in our case in a three-dimensions space. 
It has been built has an analogue to the standard Euclidian distance 
(it is normalized, so not homogeneous to a distance).\\

The NSD is introduced with the following formula:\\
\[
\rho(S_{1},S_{2})= \frac{1}{2} \sqrt {\frac{1}{N_{1}d_{2}^2} 
\cdot \sum\limits_{i=1}^{N_{1}} \rho^2(s_{1i}, S_{2}) + \frac{1}{N_{2}d_{1}^2} \cdot \sum\limits_{i=1}^{N_{2}} \rho^2(s_{2i}, S_{1})}
\]
\\
There, $S_{1}$ is the model \textit{1} set of points ($N_{1}$ points). 
For a point $s_{1i}$ in $S_{1}$, the Euclidian distance with all the 
points of $S_{2}$ is computed, and the minimal one is denoted as 
$\rho(s_{1i}, S_{2})$. 
The last parameters present in the formula is the fineness $d_{i}$. 
It correspond to the average Euclidian distance between a point of 
$S_{i}$ and its first neighbouring points.\\
The NSD respect several needs we have:
\begin{itemize}
 \item it is symmetric, $\rho(S_{1},S_{2}) = \rho(S_{2},S_{1})$
 \item it is stable, $\rho(S_{1},S_{2})$ function has a stable minimum
 \item $\rho(S_{1},S_{2}) = 0$ only if $S_{1} = S_{2}$
\end{itemize}

The calculation of the NSD is the core of our process because it is 
our indicator of the similarity between two DAM. 
This importance has also an impact on the execution time of the 
process: it is the called method which take the most of time. 
As this parameter was a key of the project, the NSD computation had to 
be optimized in term of speed.\\
The first thing done for it was to re-wrote the calculation of the NSD 
in Cython \cite{cython}. 
This programming language translates Python code to an equivalent C 
code which allows major gain in term of execution time.\\
To have again speed improvements, a second step has been performed: 
this piece of code has also been parallelized with OpenMP. 
Indeed the parallelization of some calculations allows to take 
advantage of the presence of a multi-core processor on the computer, 
which is the case on the computers used on the beamline BM29.

\section{Coarse superimposition of models}

With the data extracted of pdb files and the NSD tool, we should be 
able to calculate the dissimilarity between two models. 
The fact is that we need other tools for it because of the arrangement 
of the dammif models. 
Actually the DAM we dispose are randomly oriented, so at this moment 
of the data-analysis, the NSD between two models do not show their 
difference but only the spatial distance that separates them. 
That is why we have to first superimpose the models so that the NSD 
only point out the dissimilarity. 
Here we will define the superimposition algorithm followed in our 
program with parameters needed for it.\\

To superimpose two DAM with their set of dummy-atoms $S_{1}$ and 
$S_{2}$, we have used the algorithm defined in Kozin and Svergun 
publication \cite{supcomb} as a three-steps process.\\

The first stage consists in superimposing both models center of mass 
(COM). 
In the dummy-atoms modelization, all the dummy-atoms has the same mass 
so the COM coordinates of the model $k$ are computed as follow:
\[
x_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} x_{ik};\ \ \ 
y_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} y_{ik};\ \ \ 
z_{k}^0 = \frac{1}{N_{k}} \cdot \sum\limits_{i=1}^{N_{k}} z_{ik}
\]\\
From it, the translation to send the model $S_{k}$ to the coordinates 
origin is calculated as $T_{k}^0 = (-x_{k}^0,\ -y_{k}^0,\ -z_{k}^0)$. 
Thus, to send $S_{2}$ on $S_{1}$, we apply it two transformations: 
$T_{2}^0$ first and $(T_{1}^0)^{-1}$, where $(T_{1}^0)^{-1}$ is the 
reverse of the translation $T_{1}^0$.\\

Next come the alignment stage. 
To refine the superimposition of the two models, a rotation is 
performed on the second one. 
This rotation aims to align the principles axis of inertia of both 
DAM. 
To compute them, we first need to know the inertia matrix of the 
models $S_{k}$; and it is given by:
\[
I_{k}=
\begin{pmatrix}
 I_{11} & I_{12} & I_{13} \\
 I_{21} & I_{22} & I_{23} \\
 I_{31} & I_{32} & I_{33} \\
\end{pmatrix}
\]\\
and knowing that each point of the set $S_{k}$ is defined by
$
s_{kq}=
\begin{pmatrix}
 x_{kq}^1 \\
 x_{kq}^2 \\
 x_{kq}^3 \\
\end{pmatrix}
$\\
we have
\[
I_{ij} = \frac{1}{N_{k}} \cdot \sum\limits_{q=1}^{N_{k}} 
[\delta_{ij} \cdot \sum\limits_{l=1}^3 
 (x_{kq}^l - {x_{kq}^l}^0)^2 -  (x_{kq}^i - {x_{kq}^i}^0) 
 \cdot (x_{kq}^j - {x_{kq}^j}^0)]
\]\\
It appears that $I_{k}$ tensor is symmetric real so we know that it 
can be diagonalized; we can compute its three eigenvalues 
$\lambda_{k1} \geq \lambda_{k2} \geq \lambda_{k3}$ and the three 
eigenvectors corresponding $v_{k1},\ v_{k2},\ v_{k3}$. 
We can create a rotation matrix compound by the columns from 
eigenvectors of $I_{k}$, sorted with rising order of associated 
eigenvalues. 
If this matrix is wrote $M_{k}$, is transposed $[M_{k}]^T$ is the 
rotation matrix which align $S_{k}$ axis of inertia with the 
coordinates axis \cite{supcomb}.\\
With these rotation matrices, we are able to align two models 
following $x,\ y,\ z$ coordinates axis.\\

Before to be able to superimpose correctly $S_{2}$ on $S_{1}$, we need 
to take into account enantiomorphs. 
In fact, two enantiomorphs of a macromolecule generate the same SAXS 
curve, so that this method do not allows to segregate them. 
Thus, we do not make a difference for our data-analysis between two 
enantiomorphs; and it needs to be implemented. 
As we superimpose $S_{2}$ on $S_{1}$, we will force $S_{2}$ to be in 
the same enantiomorphic form that $S_{1}$. 
For it, $S_{2}$ and $S_{1}$ are set in their canonical position: 
center of mass and the origin of axis and inertia axis aligned with 
coordinates axis. 
Eight transformations are performed on $S_{2}$ which correspond to the 
eight possible symmetries, four for each enantiomorph. 
Transformation matrices are as follow:
\[
\begin{pmatrix}
 \pm 1 & 0 & 0 & 0 \\
 0 & \pm 1 & 0 & 0 \\
 0 & 0 & \pm 1 & 0 \\
 0 & 0 & 0 & 1
\end{pmatrix}
\]\\
And after each transformation, the NSD between $S_{1}$ and $S_{2}$ is 
computed. 
The one minimizing the NSD is kept as the good symmetry for the 
following steps. 
The rotation matrix for $S_{2}$ is now the previous one $[M_{2}]^T$ 
multiplied by the symmetry matrix, we will write it $R_{2}$.\\

In our work we chose to always align the second DAM on the first one 
initial position. 
We saw before that first $S_{2}$ is translated to put its center of 
mass on the one of $S_{1}$ using the translation 
$(T_{1}^0)^{-1} \times T_{2}^0$. 
In fact, the rotation and enantiomorph selection steps has to be 
inserted between these two translations. 
Finally, the global transformation to apply to $S_{2}$ is 
\[(T_{1}^0)^{-1} \times M_{1} \times R_{2} \times T_{2}^0\]

At the end of this process, we have two DAM whose centers of mass and 
principles axis of inertia are superimposed; we will say canonically 
superimposed.

\section{Refined superimposition}

The canonical superimposition presented before produce a good fit of 
the two models (if they are similar), but it remains a coarse 
alignment we need to optimize for a better understanding of the 
dissimilarity between two DAM. 
The core of the superimposition process remains the same, we will add 
on it the following refinement algorithm.\\
 
We have to optimize the transformation applied to $S_{2}$ to align it 
with $S_{1}$, and the criterion of improvement will be the NSD, which 
has to be as low as possible. 
For it, we use the simplex optimizer provided by the \textit{SciPy} 
library \cite{scipy}. 
But to use it, accurate numerical parameters are needed. 
What we did is to extract from the global transformation matrix 
presented before six transformation parameters: 
$(x_{0}, y_{0}, z_{0})$, 
corresponding to the total translation applied to $S_{2}$, and 
$(\alpha, \beta, \gamma)$, 
which are the Euler angles extracted from the total rotation. 
The optimizer receives this six parameters and call a function which 
returns the NSD between $S_{1}$ and $S_{2}$ after the input 
transformation has been applied to $S_{2}$. 
This call is repeat again and again, refining each time the six 
parameters values to minimize the NSD returned.\\
When these iterations reach a landing, the optimizer is stopped and 
returns the six refined parameters and the minimized NSD. 
Here, the superimposition of the two models is optimized, the final 
NSD value is assumed to reflect the actual dissimilarity between them.\\

\section{Dummy atoms models selection}

The algorithm to superimpose two DAM has been fully described, it has 
to be implemented in order to superimpose the $N$ models outgoing of 
\textit{dammif}. \\

For it, dammif pdb files are first read with the FreeSAS parser and 
usefull data are extracted as presented before. 
Some properties we introduced are then computed from it. 
At this stage we dispose of $N$ detailled DAM and all we need to 
superimpose them.\\
The superimposition is implemented to obtain a NSD table $N*N$ sized. 
Each model is represented by one row and one column. 
Each bow in the table will contain the optimized NSD between the "row" 
model and the "column" model. 
To resume, we want a $N*N$ table with 
\[
table_{ij}=NSD(S_{i},S_{j})
\]\\ 
We know that all we will have zeros on the diagonal and the table will 
be symmetric because of two properties of the NSD:
\[
NSD(S_{i},S_{i})=0.00 \Rightarrow table_{ii}=0.00
\]
\[
NSD(S_{i},S_{j}) = NSD(S_{j},S_{i}) \Rightarrow table_{ij}=table_{ji}
\]\\
Thus we can compute $\frac{N \cdot (N-1)}{2}$ optimized 
superimpositions instead of $N*N$.\\

Once the NSD table generated, we dispose of all we need to select the 
dummy-atoms models.\\
The first criteria of selection have already been presented in 
\ref{modelling} section: the $\chi^2$ and the $R-factor$. 
These two parameters are not dependent of the DAM position, they are 
computed before any movements. 
So for a gain of execution time, the test on them are computed before 
the superimposition process. 
Thus some models (say $x$) are discarded before to be superimposition, 
which allow us to skip the optimized alignment calculation to save 
time: $\frac{(N-x) \cdot (N-x-1)}{2}$ instead of 
$\frac{N \cdot (N-1)}{2}$.\\
The following criterion of selection has also been explained in 
\ref{modelling} section, the average NSD between each models and 
others. 
This variable is computed using the NSD table obtained by the process 
we know and the test is performed for each DAM remaining ($N-x$ at 
this level). 
A new series of models is discarded (or neither), that give us the 
$N-n$ acceptable DAM to follow the modelling process of EDNA. 
Moreover, the model with the lower average NSD to all other valid 
models is set as "reference model".\\

At the end of it, data are formatted to present correctly the analysis 
results. 
First, what concern DAM is saved using PDB format to be accessible to 
the next program in the pipeline: \textit{damaver}. 
The models are aligned on the reference one and are saved in this 
position, as needed by \textit{damaver}. 
Informations about the selection process are also saved to be 
available for the BM29 user. 
A piece of code has been written to create figures with $\chi^2$ and 
$R-factor$ values and thresholds, the NSD table and the average NSD 
values and threshold. 
These figures are computed using the \textit{matplotlib} Python 
library \cite{matplotlib}.\\

We saw in this chapter the way FreeSAS package has been implemented in 
order to superimpose and select dummy-atoms models from 
\textit{dammif}. 
FreeSAS provides tools to read and compute data from PDB files and to 
process them using several algorithms. 
At the end, it returns the valid DAM, superimposed and ready to be 
used by \textit{damaver}, and all data usefull for the users.
%Ajouter ici, brievement, ce qui va etre dit dans le prochain chapitre !!!


\chapter{Pratical example}%titre temporaire
    %---------------------
    % Troisieme chapitre  
    %---------------------

\section{section nb 1}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.

\section{section nb 2}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.

\section{section nb 3}

Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Praesent hendrerit mollis nisl, vel varius urna iaculis at.
Sed vel consequat enim.
Nulla sit amet blandit sapien.
Praesent sagittis varius dui non sollicitudin.
Integer ut dignissim ex, non scelerisque erat.
Etiam in tellus lobortis, condimentum purus ac, volutpat odio.
Nunc dictum maximus orci id iaculis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, 
per inceptos himenaeos.
Phasellus consequat, magna ac mattis mollis, diam nisl sagittis erat, 
quis congue nunc massa at odio.
Vivamus eleifend sollicitudin lorem, varius posuere massa cursus sit 
amet.


\chapter*{Conclusion and outlook}
\addcontentsline{toc}{chapter}{Conclusion and outlook}
    %----------------------------------------
    % Chapitre de conclusion + perspectives
    %----------------------------------------


\newpage                 %ne pas enlever (numerotation pour sommaire)
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plain}%ou autre ???
\bibliography{bibliography}
    %-------------------------
    % Bibliographie du stage
    %-------------------------


\appendix
    %---------------------
    % Annexes du rapport
    %---------------------

%a voir pour le contenu

\chapter{First appendix}

\chapter{Second appendix}


\end{document}

